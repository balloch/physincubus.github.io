{"name":"Jonathan Balloch's Homepage","tagline":"Exploring the perception and robot interaction","body":"# Welcome to my world!\r\nThis is my first webpage, and primarily here to talk about my research, development, and interests. Eventually it will link to my thought-blog and my film-blog, but as those are in a right state at the moment, they shan't show their face.\r\n\r\n## Research Interests\r\n\r\n### Background\r\nMy interest in robotics was originally sparked while getting my Masters degree from UPenn in Systems Engineering. However, in my first semester, after working with [Prof. Jonathan Fiene](http://medesign.seas.upenn.edu/) I made the switch to robotics immediately. Afterward, honed my interest into the areas of  classic machine perception working with my mentor Prof. Kostas Daniilidis, and sensor fusion working with Prof. Vijay Kumar, and became interested in how multiple robots could use each other's perception information to form a more complete understanding of the state of the world.\r\n\r\nSince graduating, I have worked as a Robotics Engineer at Intelligent Automation, Inc., a small advanced technology firm in Rockville, MD that utilizes SBIR/STTR and BAA based contracts and grants to develop cutting edge robotic solutions. Here, working chiefly under Dr. David Handelman and Dr. Mun Wai Lee on the Navy's AEODRS project developing a dual arm EOD robot for dexterous manipulation, and the DARPA MSEE project. It was especially our work on the DARPA MSEE program collaborating with Prof. Song-Chun Zhu at UCLA that helped me understand the gap that existed between vision and robot interaction that is the core of my interest today.\r\n\r\n### Core Research Interest\r\nClass level object recognition and extraction is the problem of identifying known or usable traits about a previously unknown object, and is considered by most to still be an unsolved problem in computer vision. Object level recognition and extraction, or recognizing a known object and its state, are near-solved problems that are still being perfected, but are a huge step in the right direction. One solution that is most notable is the use of Quaternion Bingham Distribution developed by Dr. Jared Glover, now CEO of CapSen Robotics, while he was at MIT (and which was used this year by MIT to place second in the Amazon Picking Challenge). However, while impressive and robust to clutter, this technique is not designed for unknown environments, which is majority of where robots will be used as they become more and more integrated into our world. \r\n\r\nIn many ways, at the core of the AI problem is knowledge representation; most of AI attempts to, effectively, memorize as much data about the world as is necessary to make informed decisions within a closed set of the world. This is why AIs like Siri and Cortana can easily schedule an appointment on your calendar or tell you a fact, but cannot always properly *interpret* your words. To some, the solution is to wait for hardware to catch up and keep digitizing more of the world until such approaches are sufficient for all normal cases. However, not only is this approach inelegant as the AI isn't really learning about the things it fails to understand, but it is unrealistic, as our world is exponentially changing as a result of the very same tech revolution that drives these AIs to be better. \r\n\r\nThe other solution that many people believe (including myself) is more appropriate is having a knowledge representation that is appropriately connected and hierarchical that just by the nature of its architecture it is constantly growing, morphing and restructuring based on what the AI experiences. If an image recognition AI program, for example, were to have a try at ImageNet with this approach, and it wasn't sure about a particularly cluttered image, instead of trying and failing to recognize the objects in the scene, it may comment about the cluttered-ness of the scene, the type of clutter (clothes, trash, plantlife, etc.), or even explain intelligently why or how the image confuses it. This is an example of a class-level recognition problem, and is just one example of how class-based heirarchical solution to knowledge representation is so powerful.\r\n\r\n...Which brings us full circle to the **\"Gap between vision and interaction\"** I have mentioned already. For robots and AI's to successfully interact with other humans, other robots, and the environment, they will need to be able to quickly adapt their knowledge, especially when it comes to perception. Moreover, robots will not only need to be able to make educated assumptions about the unknown things they perceive, but how to interact with them, which requires the *knowledge* and understanding to perceive an object's position and orientation, to know if a thing might be hard or soft, and how it might be able to manipulate it. A good example of this would be a robot that, not explicitly recognizing a unique type of glass (a bierstiefel for example) can recognize that it is standing upright or not, that it is a vessel that can hold liquid, but only when it is upright, and that it can be grasped in certain ways so that it will not fall when carried. \r\n\r\n\r\n\r\n## Other interests\r\n\r\nOutside of robotics research, I am an avid ultimate frisbee, ice hockey, and tennis player, I very much enjoy brewing beer, and playing guitar casually\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n---\r\n\r\n### Authors and Contributors\r\nShout out to @agatabcool for getting me to actually stop being lazy and create this page.\r\n\r\n---\r\n\r\n### Contact\r\nYou can find my github page (sometimes not up to date) [here](https://github.com/physincubus) or e-mail me at **jon dot balloch at gmail dot com** and weâ€™ll help you sort it out.","google":"UA-65069333-1","note":"Don't delete this file! It's used internally to help with page regeneration."}